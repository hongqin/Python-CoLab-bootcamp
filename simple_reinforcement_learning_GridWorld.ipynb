{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOOFeuwUjrSRqe115vmVvNz",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hongqin/Python-CoLab-bootcamp/blob/master/simple_reinforcement_learning_GridWorld.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iDKB-4-rkF-U",
        "outputId": "6b8c1ea9-9ae1-4a9d-cb2b-33882a9a91cb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode 50:\n",
            "['>', 'v', 'v', 'v']\n",
            "['>', '>', '>', 'v']\n",
            "['>', '>', 'v', 'v']\n",
            "['>', '>', '>', '^']\n",
            "\n",
            "Episode 100:\n",
            "['>', 'v', 'v', 'v']\n",
            "['>', '>', '>', 'v']\n",
            "['>', 'v', 'v', 'v']\n",
            "['>', '>', '>', '^']\n",
            "\n",
            "Episode 150:\n",
            "['>', 'v', 'v', 'v']\n",
            "['>', '>', '>', 'v']\n",
            "['>', 'v', 'v', 'v']\n",
            "['>', '>', '>', '^']\n",
            "\n",
            "Episode 200:\n",
            "['>', 'v', 'v', 'v']\n",
            "['>', '>', '>', 'v']\n",
            "['>', 'v', 'v', 'v']\n",
            "['>', '>', '>', '^']\n",
            "\n",
            "Episode 250:\n",
            "['>', 'v', 'v', 'v']\n",
            "['>', '>', '>', 'v']\n",
            "['>', 'v', 'v', 'v']\n",
            "['>', '>', '>', '^']\n",
            "\n",
            "Episode 300:\n",
            "['>', 'v', 'v', 'v']\n",
            "['>', '>', '>', 'v']\n",
            "['>', 'v', 'v', 'v']\n",
            "['>', '>', '>', '^']\n",
            "\n",
            "Episode 350:\n",
            "['>', 'v', 'v', 'v']\n",
            "['>', '>', '>', 'v']\n",
            "['>', 'v', 'v', 'v']\n",
            "['>', '>', '>', '^']\n",
            "\n",
            "Episode 400:\n",
            "['>', 'v', 'v', 'v']\n",
            "['>', '>', '>', 'v']\n",
            "['>', 'v', 'v', 'v']\n",
            "['>', '>', '>', '^']\n",
            "\n",
            "Episode 450:\n",
            "['>', 'v', 'v', 'v']\n",
            "['>', '>', '>', 'v']\n",
            "['>', 'v', 'v', 'v']\n",
            "['>', '>', '>', '^']\n",
            "\n",
            "Episode 500:\n",
            "['>', 'v', 'v', 'v']\n",
            "['>', '>', '>', 'v']\n",
            "['>', 'v', 'v', 'v']\n",
            "['>', '>', '>', '^']\n",
            "\n",
            "Final Q-Table:\n",
            "[[ 0.34337254  0.44171071  0.33958922  0.46089055]\n",
            " [ 0.44719766  0.56655611  0.34489787  0.56115646]\n",
            " [ 0.45517123  0.67316848  0.18441998  0.41259336]\n",
            " [ 0.35465604  0.77516117  0.30958619  0.36679538]\n",
            " [ 0.21811078  0.19974043  0.29107097  0.56596763]\n",
            " [ 0.4537175   0.65272716  0.44662872  0.673289  ]\n",
            " [ 0.54990753  0.77383647  0.56197291  0.7811    ]\n",
            " [ 0.63736129  0.89        0.67018051  0.77935409]\n",
            " [-0.10147222  0.19369502 -0.01815293  0.55495238]\n",
            " [ 0.31801308  0.77886329  0.15369206  0.64870535]\n",
            " [ 0.56091748  0.88972878  0.4662382   0.75444499]\n",
            " [ 0.74251766  1.          0.76182101  0.87667341]\n",
            " [-0.09306519  0.18578965  0.17529479  0.67303457]\n",
            " [ 0.41357452  0.56811135  0.31927573  0.88980993]\n",
            " [ 0.68033666  0.85685306  0.70898334  0.99999909]\n",
            " [ 0.          0.          0.          0.        ]]\n",
            "Final policy:\n",
            "['>', 'v', 'v', 'v']\n",
            "['>', '>', '>', 'v']\n",
            "['>', 'v', 'v', 'v']\n",
            "['>', '>', '>', '^']\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import random\n",
        "\n",
        "#############################################\n",
        "# Introduction:\n",
        "#\n",
        "# This script demonstrates a simple reinforcement learning (RL)\n",
        "# example using the Q-learning algorithm. The agent operates in a\n",
        "# GridWorld environment (a 4Ã—4 grid) where the goal is to move from\n",
        "# the starting cell (top-left) to the goal cell (bottom-right).\n",
        "#\n",
        "# The Q-learning algorithm helps the agent learn the best action to\n",
        "# take in each state by updating a Q-table using the Temporal Difference (TD)\n",
        "# error. The agent uses an epsilon-greedy strategy to balance exploration\n",
        "# (trying new actions) and exploitation (using the learned best actions).\n",
        "#\n",
        "# The code prints the evolving policy (represented with directional symbols)\n",
        "# every 50 episodes so you can observe how the agent's policy adjusts as it\n",
        "# learns. This code is designed to run in a Google Colab notebook.\n",
        "#############################################\n",
        "\n",
        "# Define the GridWorld environment class\n",
        "class GridWorld:\n",
        "    def __init__(self, size=4):\n",
        "        self.size = size                      # Grid size (size x size grid)\n",
        "        self.n_states = size * size           # Total number of states\n",
        "        self.start_state = 0                  # Starting state (top-left corner)\n",
        "        self.goal_state = self.n_states - 1   # Goal state (bottom-right corner)\n",
        "        self.state = self.start_state         # Initialize current state\n",
        "\n",
        "    def reset(self):\n",
        "        \"\"\"Reset the environment to the starting state.\"\"\"\n",
        "        self.state = self.start_state\n",
        "        return self.state\n",
        "\n",
        "    def step(self, action):\n",
        "        \"\"\"\n",
        "        Executes the given action and returns the next state, reward, and done flag.\n",
        "\n",
        "        Actions:\n",
        "          0: up\n",
        "          1: down\n",
        "          2: left\n",
        "          3: right\n",
        "\n",
        "        The agent receives a small negative reward for each step to encourage\n",
        "        finding the shortest path, and a positive reward upon reaching the goal.\n",
        "        \"\"\"\n",
        "        # Convert current state into (row, col) coordinates\n",
        "        row, col = divmod(self.state, self.size)\n",
        "\n",
        "        # Update the row, col based on the chosen action with boundary conditions:\n",
        "        if action == 0:      # Move up\n",
        "            row = max(row - 1, 0)\n",
        "        elif action == 1:    # Move down\n",
        "            row = min(row + 1, self.size - 1)\n",
        "        elif action == 2:    # Move left\n",
        "            col = max(col - 1, 0)\n",
        "        elif action == 3:    # Move right\n",
        "            col = min(col + 1, self.size - 1)\n",
        "\n",
        "        # Convert the new (row, col) back to a state index\n",
        "        new_state = row * self.size + col\n",
        "\n",
        "        # Determine the reward:\n",
        "        #   +1 for reaching the goal, -0.1 for each step otherwise.\n",
        "        reward = 1 if new_state == self.goal_state else -0.1\n",
        "\n",
        "        # Check if the goal has been reached\n",
        "        done = new_state == self.goal_state\n",
        "\n",
        "        # Update the current state\n",
        "        self.state = new_state\n",
        "\n",
        "        return new_state, reward, done\n",
        "\n",
        "#############################################\n",
        "# Q-Learning Setup:\n",
        "#\n",
        "# We initialize a Q-table where each row corresponds to a state\n",
        "# and each column corresponds to an action. The Q-values represent\n",
        "# the expected reward for taking an action in a state.\n",
        "#############################################\n",
        "\n",
        "env = GridWorld(size=4)\n",
        "n_actions = 4\n",
        "q_table = np.zeros((env.n_states, n_actions))  # Initialize Q-table with zeros\n",
        "\n",
        "# Q-learning hyperparameters\n",
        "alpha = 0.1         # Learning rate: controls how much new information overrides old.\n",
        "gamma = 0.99        # Discount factor: weighs future rewards relative to immediate ones.\n",
        "epsilon = 1.0       # Exploration rate: probability of choosing a random action.\n",
        "epsilon_decay = 0.995  # Decay rate for epsilon after each episode.\n",
        "min_epsilon = 0.01  # Minimum value for epsilon to ensure some exploration.\n",
        "num_episodes = 500  # Total number of episodes for training\n",
        "\n",
        "def choose_action(state):\n",
        "    \"\"\"\n",
        "    Epsilon-greedy strategy to choose an action:\n",
        "      - With probability 'epsilon', choose a random action (exploration).\n",
        "      - Otherwise, choose the action with the highest Q-value (exploitation).\n",
        "    \"\"\"\n",
        "    if random.random() < epsilon:\n",
        "        return random.randint(0, n_actions - 1)\n",
        "    else:\n",
        "        return np.argmax(q_table[state])\n",
        "\n",
        "def print_policy():\n",
        "    \"\"\"\n",
        "    Extracts the best action (policy) from the Q-table and prints it in a grid.\n",
        "\n",
        "    The actions are mapped to symbols:\n",
        "      0: up (^)\n",
        "      1: down (v)\n",
        "      2: left (<)\n",
        "      3: right (>)\n",
        "    \"\"\"\n",
        "    # Derive the policy by choosing the action with the highest Q-value at each state.\n",
        "    policy = np.array([np.argmax(q_table[s]) for s in range(env.n_states)])\n",
        "    symbols = {0: '^', 1: 'v', 2: '<', 3: '>'}  # Mapping from action to symbol\n",
        "    policy_symbols = [symbols[a] for a in policy]\n",
        "\n",
        "    # Print the policy grid row by row\n",
        "    for i in range(env.size):\n",
        "        print(policy_symbols[i*env.size:(i+1)*env.size])\n",
        "    print()\n",
        "\n",
        "#############################################\n",
        "# Training Loop:\n",
        "#\n",
        "# For each episode, the agent starts at the beginning of the grid.\n",
        "# It then chooses actions based on the current policy (with some exploration),\n",
        "# updates the Q-table using the Q-learning update rule, and moves to the next state.\n",
        "#\n",
        "# The Q-learning update rule is:\n",
        "#   Q(state, action) = Q(state, action) + alpha * [reward + gamma * max(Q(next_state)) - Q(state, action)]\n",
        "#\n",
        "# The policy is printed every 50 episodes to show how it evolves over time.\n",
        "#############################################\n",
        "\n",
        "for episode in range(1, num_episodes + 1):\n",
        "    state = env.reset()  # Reset environment at the beginning of each episode\n",
        "    done = False\n",
        "    while not done:\n",
        "        # Choose an action using epsilon-greedy strategy\n",
        "        action = choose_action(state)\n",
        "\n",
        "        # Execute the action and observe the next state, reward, and whether the goal is reached\n",
        "        next_state, reward, done = env.step(action)\n",
        "\n",
        "        # Identify the best action from the next state (for estimating the future reward)\n",
        "        best_next_action = np.argmax(q_table[next_state])\n",
        "\n",
        "        # Q-learning update:\n",
        "        # Calculate the TD target (reward plus discounted estimated future reward)\n",
        "        td_target = reward + gamma * q_table[next_state, best_next_action]\n",
        "\n",
        "        # Calculate the TD error (difference between the target and current Q-value)\n",
        "        td_error = td_target - q_table[state, action]\n",
        "\n",
        "        # Update the Q-value for the current state and action\n",
        "        q_table[state, action] += alpha * td_error\n",
        "\n",
        "        # Transition to the next state\n",
        "        state = next_state\n",
        "\n",
        "    # Decay epsilon after each episode to reduce exploration over time\n",
        "    epsilon = max(min_epsilon, epsilon * epsilon_decay)\n",
        "\n",
        "    # Every 50 episodes, print the current policy so you can see the agent's progress\n",
        "    if episode % 50 == 0:\n",
        "        print(f\"Episode {episode}:\")\n",
        "        print_policy()\n",
        "\n",
        "#############################################\n",
        "# Final Interpretation:\n",
        "#\n",
        "# After the training loop, the Q-table contains the estimated rewards for\n",
        "# each state-action pair. The final policy (the best action for each state)\n",
        "# is derived from the Q-table. This policy should represent an efficient\n",
        "# path from the start state to the goal state.\n",
        "#############################################\n",
        "\n",
        "print(\"Final Q-Table:\")\n",
        "print(q_table)\n",
        "print(\"Final policy:\")\n",
        "print_policy()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rxXdmdKxkGyN"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}